syntax = "proto3";

// Service for offloading computationally complex NLP tasks.
package arg_services.nlp.v1;

import "google/api/annotations.proto";
import "google/protobuf/struct.proto";

service NlpService {
  // Compute embeddings (i.e., vectors) for strings.
  rpc Vectors(VectorsRequest) returns (VectorsResponse) {
    option (google.api.http) = {
      post: "/nlp/v1/vectors"
      body: "*"
    };
  }
  // Compute the similarity score between two strings.
  rpc Similarities(SimilaritiesRequest) returns (SimilaritiesResponse) {
    option (google.api.http) = {
      post: "/nlp/v1/similarities"
      body: "*"
    };
  }
  // Process strings by spacy and return them as [binary data](https://spacy.io/api/docbin).
  // Locally, spacy can restore this data **without** loading the underlying NLP models into the main memory.
  // Allows one to retrieve all computed attributes (e.g., POS tags, sentences), but can only be used by Python programs.
  rpc DocBin(DocBinRequest) returns (DocBinResponse) {
    option (google.api.http) = {
      post: "/nlp/v1/docbin"
      body: "*"
    };
  }
}

// Common message for configuring spacy.
message NlpConfig {
  // Any language supported by spacy (e.g., `en`).
  // [Reference](https://spacy.io/usage/models#languages).
  string language = 1;
  // Name of the trained spacy pipeline (e.g., `en_core_web_lg`).
  // If empty, a blank spacy model will be used (e.g., if you only need embeddings and provide custom `embedding_models`.
  // [Example: English models](https://spacy.io/models/en).
  string spacy_model = 2;
  // List of embeddings to use for computing word/sentence vectors.
  // If given, these embeddings will **override** the embeddings of the specified `spacy_model`.
  // Multiple models are concatenated to each other, increasing the length of the resulting vector.
  repeated EmbeddingModel embedding_models = 3;
  // Mathematical function to determine a similarity score given two strings.
  SimilarityMethod similarity_method = 4;
}

message SimilaritiesRequest {
  // Spacy config.
  NlpConfig config = 1;
  // List of string pairs to compare.
  repeated TextTuple text_tuples = 2;
  // Implementation-specific information can be encoded here
  google.protobuf.Struct extras = 15;
}

message SimilaritiesResponse {
  // List of similarities ordered just like the original `text_tuples`.
  repeated double similarities = 1;
  // Implementation-specific information can be encoded here
  google.protobuf.Struct extras = 15;
}

// Possible methods to compute the similarity between two vectors.
enum SimilarityMethod {
  // If not given, the implementation defaults to cosine similarity.
  SIMILARITY_METHOD_UNSPECIFIED = 0;
  // Cosine similarity. [Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity).
  SIMILARITY_METHOD_COSINE = 1;
  // DynaMax Jaccard. [Paper](https://arxiv.org/abs/1904.13264), [Code](https://github.com/babylonhealth/fuzzymax/blob/master/similarity/fuzzy.py).
  SIMILARITY_METHOD_DYNAMAX_JACCARD = 2;
  // MaxPool Jaccard. [Paper](https://arxiv.org/abs/1904.13264), [Code](https://github.com/babylonhealth/fuzzymax/blob/master/similarity/fuzzy.py).
  SIMILARITY_METHOD_MAXPOOL_JACCARD = 3;
  // DynaMax Dice. [Paper](https://arxiv.org/abs/1904.13264), [Code](https://github.com/babylonhealth/fuzzymax/blob/master/similarity/fuzzy.py).
  SIMILARITY_METHOD_DYNAMAX_DICE = 4;
  // DynaMax Otsuka. [Paper](https://arxiv.org/abs/1904.13264), [Code](https://github.com/babylonhealth/fuzzymax/blob/master/similarity/fuzzy.py).
  SIMILARITY_METHOD_DYNAMAX_OTSUKA = 5;
  // Word Mover's Distance [Gensim Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html).
  SIMILARITY_METHOD_WMD = 6;
  // Levenshtein distance. [Wikipedia](https://en.wikipedia.org/wiki/Levenshtein_distance).
  SIMILARITY_METHOD_EDIT = 7;
  // Jaccard similarity. [Wikipedia](https://en.wikipedia.org/wiki/Jaccard_index).
  SIMILARITY_METHOD_JACCARD = 8;
  // Angular distance. [Wikipedia](https://en.wikipedia.org/wiki/Angular_distance).
  SIMILARITY_METHOD_ANGULAR = 9;
  // Manhattan distance. [Wikipedia](https://en.wikipedia.org/wiki/Taxicab_geometry).
  SIMILARITY_METHOD_MANHATTAN = 10;
  // Euclidean distance. [Wikipedia](https://en.wikipedia.org/wiki/Euclidean_distance).
  SIMILARITY_METHOD_EUCLIDEAN = 11;
  // Dot product. [Wikipedia](https://en.wikipedia.org/wiki/Dot_product).
  SIMILARITY_METHOD_DOT = 12;
}

// Store a pair of strings.
message TextTuple {
  string text1 = 1;
  string text2 = 2;
}

// Wrapper message to encode a list of strings that can also be `null`.
message Strings {
  repeated string values = 1;
}

message DocBinRequest {
  // Spacy config.
  NlpConfig config = 1;
  // List of strings to be processed.
  repeated string texts = 2;
  // Attributes that shall be included in the DocBin object.
  // Defaults to `("ORTH", "TAG", "HEAD", "DEP", "ENT_IOB", "ENT_TYPE", "ENT_KB_ID", "LEMMA", "MORPH", "POS")`.
  // Possible values: `("IS_ALPHA", "IS_ASCII", "IS_DIGIT", "IS_LOWER", "IS_PUNCT", "IS_SPACE", "IS_TITLE", "IS_UPPER", "LIKE_URL", "LIKE_NUM", "LIKE_EMAIL", "IS_STOP", "IS_OOV_DEPRECATED", "IS_BRACKET", "IS_QUOTE", "IS_LEFT_PUNCT", "IS_RIGHT_PUNCT", "IS_CURRENCY", "ID", "ORTH", "LOWER", "NORM", "SHAPE", "PREFIX", "SUFFIX", "LENGTH", "CLUSTER", "LEMMA", "POS", "TAG", "DEP", "ENT_IOB", "ENT_TYPE", "ENT_ID", "ENT_KB_ID", "HEAD", "SENT_START", "SENT_END", "SPACY", "PROB", "LANG", "MORPH", "IDX")`.
  // [Documentation](https://spacy.io/api/token#attributes).
  optional Strings attributes = 3;
  // List of pipeline components that shall be enabled/disabled when processing documents.
  // If only certain attributed (e.g. POS tags) are relevant, one can enhance the performance by selecting components.
  // **Important**: There may be custom spacy components that are required for some of the functionality of the NLP service.
  // In the reference implementation, this applies to the components `embedding_models` and `similarity_method`.
  oneof pipes {
    Strings enabled_pipes = 4;
    Strings disabled_pipes = 5;
  }
  // List of vectors that shall be saved in the `DocBin` object.
  // The computation is time-consuming, so you should only specify the embeddings you actually use!
  repeated EmbeddingLevel embedding_levels = 6;
  // Implementation-specific information can be encoded here
  google.protobuf.Struct extras = 15;
}

message DocBinResponse {
  // Serialized [`DocBin`](https://spacy.io/api/docbin) object
  bytes docbin = 1;
  // Implementation-specific information can be encoded here
  google.protobuf.Struct extras = 15;
}

message VectorsRequest {
  // Spacy config.
  NlpConfig config = 1;
  // List of strings that shall be embedded (i.e., converted to vectors).
  repeated string texts = 2;
  // List of vectors that shall be returned.
  // The computation is time-consuming, so you should only specify the embeddings you actually use!
  repeated EmbeddingLevel embedding_levels = 3;
  // Implementation-specific information can be encoded here
  google.protobuf.Struct extras = 15;
}

message VectorsResponse {
  // List of vectors whose order corresponds to the one of `texts`.
  repeated VectorResponse vectors = 1;
  // Implementation-specific information can be encoded here
  google.protobuf.Struct extras = 15;
}

// Container object that includes vectors for all levels specified in `embedding_levels`.
message VectorResponse {
  // One vector for the whole string.
  Vector document = 1;
  // Vectors for all tokens in the string.
  repeated Vector tokens = 2;
  // Vectors for all sentences found in the string.
  repeated Vector sentences = 3;
  // Implementation-specific information can be encoded here
  google.protobuf.Struct extras = 15;
}

// Container for storing a vector as a list of floats.
message Vector {
  repeated double vector = 1;
}

enum EmbeddingLevel {
  // In the default case, no vector is computed.
  EMBEDDING_LEVEL_UNSPECIFIED = 0;
  // Compute one vector for the whole string.
  EMBEDDING_LEVEL_DOCUMENT = 1;
  // Compute vectors for all tokens in the string.
  EMBEDDING_LEVEL_TOKENS = 2;
  // Compute vectors for all sentences found in the string.
  EMBEDDING_LEVEL_SENTENCES = 3;
}

// Specification of one model that is used to generate embeddings for strings.
message EmbeddingModel {
  // Each embedding has to be implemented, thus this enum is used to select the correct one.
  EmbeddingType model_type = 1;
  // You have to specify the name of the model that should be used by the selected impelemtation (i.e., `model_type`).
  // We provide links to exemplary models for each implementation in the documentation of `EmbeddingType`.
  string model_name = 2;
  // In case the selected model is not capable of directly creating sentence embeddings, you have to select a pooling strategy.
  // You can either use a standard function (`pooling_type`) or compute the power mean (`pmean`).
  oneof pooling {
    // Standard pooling functions like mean, min, max.
    Pooling pooling_type = 3;
    // Power mean (or generalized mean).
    // This method allows you to alter the computation of the mean representation.
    // Special cases include arithmetic mean (p = 1), geometric mean (p = 0), harmonic mean (p = -1), minimum (p = -∞), maximum (p = ∞).
    // [Wikipedia](https://en.wikipedia.org/wiki/Generalized_mean).
    // [Paper](https://arxiv.org/abs/1803.01400).
    double pmean = 4;
  }
}

enum Pooling {
  // IN the default case, the arithmetic mean should be used.
  POOLING_UNSPECIFIED = 0;
  // Arithmetic mean of all elements. [Wikipedia](https://en.wikipedia.org/wiki/Arithmetic_mean).
  POOLING_MEAN = 1;
  // Maximum element of vector. [Wikipedia](https://en.wikipedia.org/wiki/Maximum).
  POOLING_MAX = 2;
  // Minimum element of vector. [Wikipedia](https://en.wikipedia.org/wiki/Minimum).
  POOLING_MIN = 3;
  // Sum of all elements.
  POOLING_SUM = 4;
  // First element of vector.
  POOLING_FIRST = 5;
  // Last element of vector.
  POOLING_LAST = 6;
  // Median element of vector. [Wikipedia](https://en.wikipedia.org/wiki/Median).
  POOLING_MEDIAN = 7;
  // Geometirc mean of all elements. [Wikipedia](https://en.wikipedia.org/wiki/Geometric_mean).
  POOLING_GMEAN = 8;
  // Harmonic mean of all elements. [Wikipedia](https://en.wikipedia.org/wiki/Harmonic_mean).
  POOLING_HMEAN = 9;
}

enum EmbeddingType {
  // In the default case, no embedding is computed.
  EMBEDDING_TYPE_UNSPECIFIED = 0;
  // [Spacy](https://spacy.io/models).
  EMBEDDING_TYPE_SPACY = 1;
  // [HuggingFace Transformers](https://huggingface.co/models).
  EMBEDDING_TYPE_TRANSFORMERS = 2;
  // [UKPLab Sentence Transformers](https://www.sbert.net/docs/pretrained_models.html)
  EMBEDDING_TYPE_SENTENCE_TRANSFORMERS = 3;
  // Tensorflow Hub. Example: [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4).
  EMBEDDING_TYPE_TENSORFLOW_HUB = 4;
  // [OpenAI](https://platform.openai.com/docs/models/embeddings).
  EMBEDDING_TYPE_OPENAI = 5;
  // [Ollama](https://ollama.com/blog/embedding-models)
  EMBEDDING_TYPE_OLLAMA = 6;
  // [Cohere](https://docs.cohere.com/reference/embed)
  EMBEDDING_TYPE_COHERE = 7;
  // [VoyageAI](https://docs.voyageai.com/docs/embeddings)
  EMBEDDING_TYPE_VOYAGEAI = 8;
}
