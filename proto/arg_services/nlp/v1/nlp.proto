syntax = "proto3";

package arg_services.nlp.v1;
option java_package = "de.uni_trier.recap.arg_services.nlp.v1";
import "google/protobuf/field_mask.proto";

// Service for offloading computationally complex NLP tasks.
service NlpService {
    // Compute embeddings (i.e., vectors) for strings.
    rpc Vectors (VectorsRequest) returns (VectorsResponse);
    // Compute the similarity score between two strings.
    rpc Similarities (SimilaritiesRequest) returns (SimilaritiesResponse);
    // Process strings by spacy and return them as [binary data](https://spacy.io/api/docbin).
    // Locally, spacy can restore this data **without** loading the underlying NLP models into the main memory.
    // Allows one to retrieve all computed attributes (e.g., POS tags, sentences), but can only be used by Python programs.
    rpc DocBin (DocBinRequest) returns (DocBinResponse);
}

// Common message for configuring spacy.
message NlpConfig {
    // Any language supported by spacy (e.g., `en`).
    // [Reference](https://spacy.io/usage/models#languages).
    string language = 1;
    // Name of the trained spacy pipeline (e.g., `en_core_web_lg`).
    // If empty, a blank spacy model will be used (e.g., if you only need embeddings and provide custom `embedding_models`.
    // [Example: English models](https://spacy.io/models/en).
    string spacy_model = 2;
    // List of embeddings to use for computing word/sentence vectors.
    // If given, these embeddings will **override** the embeddings of the specified `spacy_model`.
    // Multiple models are concatenated to each other, increasing the length of the resulting vector.
    repeated EmbeddingModel embedding_models = 3;
    // Mathematical function to determine a similarity score given two strings.
    SimilarityMethod similarity_method = 4;
}

message SimilaritiesRequest {
    // Spacy config.
    NlpConfig config = 1;
    // List of string pairs to compare.
    repeated TextTuple text_tuples = 2;
}

message SimilaritiesResponse {
    // List of similarities ordered just like the original `text_tuples`.
    repeated double similarities = 1;
}

// Possible methods to compute the similarity between two vectors.
enum SimilarityMethod {
    // If not given, the implementation defaults to cosine similarity.
    SIMILARITY_METHOD_UNSPECIFIED = 0;
    // Cosine similarity. [Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity).
    SIMILARITY_METHOD_COSINE = 1;
    // DynaMax Jaccard. [Paper](https://arxiv.org/abs/1904.13264), [Code](https://github.com/babylonhealth/fuzzymax/blob/master/similarity/fuzzy.py).
    SIMILARITY_METHOD_DYNAMAX_JACCARD = 2;
    // MaxPool Jaccard. [Paper](https://arxiv.org/abs/1904.13264), [Code](https://github.com/babylonhealth/fuzzymax/blob/master/similarity/fuzzy.py).
    SIMILARITY_METHOD_MAXPOOL_JACCARD = 3;
    // DynaMax Dice. [Paper](https://arxiv.org/abs/1904.13264), [Code](https://github.com/babylonhealth/fuzzymax/blob/master/similarity/fuzzy.py).
    SIMILARITY_METHOD_DYNAMAX_DICE = 4;
    // DynaMax Otsuka. [Paper](https://arxiv.org/abs/1904.13264), [Code](https://github.com/babylonhealth/fuzzymax/blob/master/similarity/fuzzy.py).
    SIMILARITY_METHOD_DYNAMAX_OTSUKA = 5;
    // Word Mover's Distance [Gensim Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html).
    SIMILARITY_METHOD_WMD = 6;
    // Levenshtein distance. [Wikipedia](https://en.wikipedia.org/wiki/Levenshtein_distance).
    SIMILARITY_METHOD_EDIT = 7;
    // Jaccard similarity. [Wikipedia](https://en.wikipedia.org/wiki/Jaccard_index).
    SIMILARITY_METHOD_JACCARD = 8;
    // Angular distance. [Wikipedia](https://en.wikipedia.org/wiki/Angular_distance).
    SIMILARITY_METHOD_ANGULAR = 9;
}

// Store a pair of strings.
message TextTuple {
    string text1 = 1;
    string text2 = 2;
}

// Wrapper message to encode a list of strings that can also be `null`.
message Strings {
    repeated string values = 1;
}

message DocBinRequest {
    // Spacy config.
    NlpConfig config = 1;
    // List of strings to be processed.
    repeated string texts = 2;
    // Attributes that shall be included in the DocBin object.
    // Defaults to `("ORTH", "TAG", "HEAD", "DEP", "ENT_IOB", "ENT_TYPE", "ENT_KB_ID", "LEMMA", "MORPH", "POS")`.
    // Possible values: `("IS_ALPHA", "IS_ASCII", "IS_DIGIT", "IS_LOWER", "IS_PUNCT", "IS_SPACE", "IS_TITLE", "IS_UPPER", "LIKE_URL", "LIKE_NUM", "LIKE_EMAIL", "IS_STOP", "IS_OOV_DEPRECATED", "IS_BRACKET", "IS_QUOTE", "IS_LEFT_PUNCT", "IS_RIGHT_PUNCT", "IS_CURRENCY", "ID", "ORTH", "LOWER", "NORM", "SHAPE", "PREFIX", "SUFFIX", "LENGTH", "CLUSTER", "LEMMA", "POS", "TAG", "DEP", "ENT_IOB", "ENT_TYPE", "ENT_ID", "ENT_KB_ID", "HEAD", "SENT_START", "SENT_END", "SPACY", "PROB", "LANG", "MORPH", "IDX")`.
    // [Documentation](https://spacy.io/api/token#attributes).
    optional Strings attributes = 3;
    // List of pipeline components that shall be enabled/disabled when processing documents.
    // If only certain attributed (e.g. POS tags) are relevant, one can enhance the performance by selecting components.
    // **Important**: There may be custom spacy components that are required for some of the functionality of the NLP service.
    // In the reference implementation, this applies to the components `embedding_models` and `similarity_method`.
    oneof pipes {
        Strings enabled_pipes = 4;
        Strings disabled_pipes = 5;
    }
    // List of vectors that shall be saved in the `DocBin` object.
    // The computation is time-consuming, so you should only specify the embeddings you actually use!
    repeated EmbeddingLevel embedding_levels = 6;
}

message DocBinResponse {
    // Serialized [`DocBin`](https://spacy.io/api/docbin) object
    bytes docbin = 1;
}

message VectorsRequest {
    // Spacy config.
    NlpConfig config = 1;
    // List of strings that shall be embedded (i.e., converted to vectors).
    repeated string texts = 2;
    // List of vectors that shall be returned.
    // The computation is time-consuming, so you should only specify the embeddings you actually use!
    repeated EmbeddingLevel embedding_levels = 3;
}

message VectorsResponse {
    // List of vectors whose order corresponds to the one of `texts`.
    repeated VectorResponse vectors = 1;
}

// Container object that includes vectors for all levels specified in `embedding_levels`.
message VectorResponse {
    // One vector for the whole string.
    Vector document = 1;
    // Vectors for all tokens in the string.
    repeated Vector tokens = 2;
    // Vectors for all sentences found in the string.
    repeated Vector sentences = 3;
}

// Container for storing a vector as a list of floats.
message Vector {
    repeated double vector = 1;
}

enum EmbeddingLevel {
    // In the default case, no vector is computed.
    EMBEDDING_LEVEL_UNSPECIFIED = 0;
    // Compute one vector for the whole string.
    EMBEDDING_LEVEL_DOCUMENT = 1;
    // Compute vectors for all tokens in the string.
    EMBEDDING_LEVEL_TOKENS = 2;
    // Compute vectors for all sentences found in the string.
    EMBEDDING_LEVEL_SENTENCES = 3;
}

// Specification of one model that is used to generate embeddings for strings.
message EmbeddingModel {
    // Each embedding has to be implemented, thus this enum is used to select the correct one.
    EmbeddingType model_type = 1;
    // You have to specify the name of the model that should be used by the selected impelemtation (i.e., `model_type`).
    // We provide links to exemplary models for each implementation in the documentation of `EmbeddingType`.
    string model_name = 2;
    // In case the selected model is not capable of directly creating sentence embeddings, you have to select a pooling strategy.
    // You can either use a standard function (`pooling_type`) or compute the power mean (`pmean`).
    oneof pooling {
        // Standard pooling functions like mean, min, max.
        Pooling pooling_type = 3;
        // Power mean (or generalized mean).
        // This method allows you to alter the computation of the mean representation.
        // Special cases include arithmetic mean (p = 1), geometric mean (p = 0), harmonic mean (p = -1), minimum (p = -∞), maximum (p = ∞).
        // [Wikipedia](https://en.wikipedia.org/wiki/Generalized_mean).
        // [Paper](https://arxiv.org/abs/1803.01400).
        double pmean = 4;
    }
}

enum Pooling {
    // IN the default case, the arithmetic mean should be used.
    POOLING_UNSPECIFIED = 0;
    // Arithmetic mean of all elements. [Wikipedia](https://en.wikipedia.org/wiki/Arithmetic_mean).
    POOLING_MEAN = 1;
    // Maximum element of vector. [Wikipedia](https://en.wikipedia.org/wiki/Maximum).
    POOLING_MAX = 2;
    // Minimum element of vector. [Wikipedia](https://en.wikipedia.org/wiki/Minimum).
    POOLING_MIN = 3;
    // Sum of all elements.
    POOLING_SUM = 4;
    // First element of vector.
    POOLING_FIRST = 5;
    // Last element of vector.
    POOLING_LAST = 6;
    // Median element of vector. [Wikipedia](https://en.wikipedia.org/wiki/Median).
    POOLING_MEDIAN = 7;
    // Geometirc mean of all elements. [Wikipedia](https://en.wikipedia.org/wiki/Geometric_mean).
    POOLING_GMEAN = 8;
    // Harmonic mean of all elements. [Wikipedia](https://en.wikipedia.org/wiki/Harmonic_mean).
    POOLING_HMEAN = 9;
}

enum EmbeddingType {
    // In the default case, no embedding is computed.
    EMBEDDING_TYPE_UNSPECIFIED = 0;
    // [Spacy](https://spacy.io/models).
    EMBEDDING_TYPE_SPACY = 1;
    // [HuggingFace Transformers](https://huggingface.co/models).
    EMBEDDING_TYPE_TRANSFORMERS = 2;
    // [UKPLab Sentence Transformers](https://www.sbert.net/docs/pretrained_models.html)
    EMBEDDING_TYPE_SENTENCE_TRANSFORMERS = 3;
    // Tensorflow Hub. Example: [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4).
    EMBEDDING_TYPE_TENSORFLOW_HUB = 4;
}
